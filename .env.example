# ===============================================================================
# Environment Variables for Legal Event Extraction
# ===============================================================================
# Copy this file to .env and add your API keys
#
# Supported providers: LangExtract (Gemini), OpenRouter, OpenCode Zen, OpenAI, Anthropic
# See docs/reference/configuration.md for model comparisons and pricing
# ===============================================================================

# ====================
# Event Extraction Providers
# ====================
# Based on 2025-10-03 testing:
#
# DIGITAL PDFs (docs/benchmarks/2025-10-03-manual-comparison.md):
# - OpenRouter (GPT-4o-mini): ⭐ Best overall (8/10 quality, ~$0.015/doc, 14s)
# - Anthropic (Claude 3 Haiku): Speed/Cost (7/10 quality, $0.003/doc, 4.4s)
# - OpenAI (GPT-4o-mini): Quality (8/10 quality, $0.03/doc, 18s)
# - LangExtract (Gemini 2.0 Flash): Completeness (6/10 quality, ~$0.01/doc, 36s)
#
# SCANNED PDFs (docs/benchmarks/2025-10-03-ocr-comparison.md):
# - Anthropic (Claude 3 Haiku): ⭐ OCR Champion (10/10 quality, $0.0005/doc, 2s)
# - OpenAI (GPT-4o-mini): Maximum detail (10/10 quality, $0.0039/doc, 6s)
# - OpenRouter (GPT-4o-mini): Consistent (10/10 quality, ~$0.008/doc, 8s)
# - LangExtract (Gemini 2.0 Flash): Comprehensive (7/10 quality, ~$0.002/doc, 4s)
#
# KEY FINDING: ✅ OCR does NOT degrade extraction quality - Docling OCR is production-ready!

# OpenRouter (Unified Multi-Provider API) ⭐ RECOMMENDED DEFAULT
# OPENROUTER_API_KEY=your_openrouter_api_key_here
# OPENROUTER_MODEL=openai/gpt-oss-120b
#
# Curated models (Oct 2025 testing - 10/10, 9/10, or 7/10 quality):
# - openai/gpt-oss-120b: $0.31/M, 128K (⭐ OSS default: Apache 2.0, self-hostable, 10/10)
# - deepseek/deepseek-r1-distill-llama-70b: $0.03/M, 128K (50x cheaper, 10/10 quality!)
# - openai/gpt-4o-mini: $0.15/M, 128K context (budget proprietary option, 9/10)
# - qwen/qwq-32b: $0.115/M, 128K (ultra-cheap, 7/10, ⚠️ may miss events on complex docs)
# - anthropic/claude-3-haiku: $0.25/M, 200K (fastest: 4.4s, long docs, 10/10)
# - anthropic/claude-3-5-sonnet: $3/M, 200K (premium quality, 10/10, 50+ page contracts)
# - meta-llama/llama-3.3-70b-instruct: $0.60/M, 128K (open source option, 10/10)
#
# ⚠️ Excluded models (failed Oct 2025 JSON tests): All Gemini variants, Cohere, Perplexity
#
# ⚠️ Additional exclusions (Oct 5-13 OSS model testing):
# - openai/gpt-oss-20b: Returns empty responses (smaller variant fundamentally broken, 1/10)
# - mistralai/mistral-small-24b-instruct-2501: Weak real-doc extraction (7/10, 1 event vs ≥3 expected)
#
# ✅ Oct 13 DISCOVERY: openai/gpt-oss-120b WORKS (10/10) - larger variant succeeds with prompt-based JSON!
# - Oct 5 test failed due to unconditional response_format parameter in test script
# - Production adapter conditionally adds response_format only for compatible models
# - GPT-OSS-120B works perfectly via prompt-based JSON approach (no native JSON mode needed)
#
# ✅ Oct 5 adapter fix: Added conditional JSON mode support (prompt-based fallback)
# - Qwen QwQ 32B now works (7/10 quality, was failing before)
# - OpenRouter adapter now tries native JSON mode first, falls back to prompt-based
# - Automatically strips markdown wrappers from responses
#
# Key finding: Native JSON mode (response_format) is NOT mandatory - many models work via prompts.
# Testing with real legal documents remains essential for curation.

# Anthropic Direct API (Speed/Cost Champion)
# ANTHROPIC_API_KEY=your_anthropic_api_key_here
# ANTHROPIC_MODEL=claude-3-haiku-20240307

# OpenAI Direct API (Quality Champion)
# OPENAI_API_KEY=your_openai_api_key_here
# OPENAI_MODEL=gpt-4o-mini

# LangExtract (Google Gemini) - Completeness Champion
GOOGLE_API_KEY=your_google_api_key_here
GEMINI_API_KEY=your_google_api_key_here

# OpenCode Zen (Legal AI Gateway) - ⚠️ Currently unstable
# OPENCODEZEN_API_KEY=your_opencode_zen_api_key_here
# OPENCODEZEN_MODEL=grok-code

# ====================
# DeepSeek Direct API (✅ Available - Week 3)
# ====================
# Pricing: $0.27/M input, $1.10/M output
# Signup: https://platform.deepseek.com (instant, no approval)
# DEEPSEEK_API_KEY=your_deepseek_api_key_here
# DEEPSEEK_MODEL=deepseek-chat
# DEEPSEEK_BASE_URL=https://api.deepseek.com/v1

# ====================
# 3-Judge Panel System (Phase 4 - Advanced Evaluation)
# ====================
# Premium reasoning models for robust legal event extraction evaluation
# Uses GPT-5, Claude Opus 4.1, and Gemini 2.5 Pro with consensus mechanism
#
# Requirements (all 3 API keys):
# - OPENAI_API_KEY (GPT-5 with reasoning_effort="high")
# - ANTHROPIC_API_KEY (Claude Opus 4.1 with extended thinking)
# - GEMINI_API_KEY (Gemini 2.5 Pro with built-in thinking)
#
# Cost: ~$0.25 per document (vs $0.01 single judge)
# Validation: ✅ Matches Phase 2 manual evaluation
# Inter-judge agreement: 0.974 (exceptional)
# Confidence level: HIGH
#
# Run validation: uv run python scripts/validate_judge_panel.py
#
# All keys already configured above ✅

# ====================
# Ground Truth Model Selection (Premium Models for Reference Datasets)
# ====================
# The UI now supports selecting premium models for creating ground truth extraction datasets.
# Ground truth datasets serve as reference benchmarks for testing cheaper production models.
#
# Use cases:
# - Create reference extraction datasets with highest-quality models
# - Validate production model outputs against premium model extractions
# - Quality assurance for critical legal documents
# - Benchmark testing of new models or prompt versions
#
# Available ground truth models (by provider):
#
# ANTHROPIC (Direct API):
#   - claude-sonnet-4-5 (Tier 1): $3/M input • $15/M output • 200K context
#     → "Best coding model in the world" (Sep 2025), recommended for ground truth
#     → Best balance of quality, cost, and speed
#   - claude-opus-4 (Tier 3): $15/M input • $75/M output • 200K context
#     → Highest quality model (May 2025), best for complex reasoning
#     → Use for quality validation of Tier 1 outputs
#
# OPENAI (Direct API):
#   - gpt-5 (Tier 2): $TBD • 128K context (Aug 2025)
#     → Latest flagship model, best for coding and reasoning
#     → Pricing pending official announcement
#
# LANGEXTRACT (Google Gemini):
#   - gemini-2.5-pro (Tier 2): $TBD • 2M context (Jun 2025)
#     → Most intelligent AI model from Google
#     → Ideal for long documents (50+ pages) due to 2M context window
#
# How to use:
# 1. Run the Streamlit app: uv run streamlit run app.py
# 2. Select a provider (Anthropic, OpenAI, or LangExtract)
# 3. Choose a ground truth model from the dropdown
# 4. Process documents to create reference extraction dataset
# 5. Export results for comparison with production model outputs
#
# Tier system:
# - Tier 1 (Recommended): Claude Sonnet 4.5 - best balance for most use cases
# - Tier 2 (Alternative): GPT-5, Gemini 2.5 Pro - pending pricing or specialized use
# - Tier 3 (Validation): Claude Opus 4 - highest quality, use sparingly for validation
#
# Cost comparison (estimated per 15-page document):
# - Production models: $0.001-0.005/doc (GPT-4o-mini, Claude Haiku, Gemini 2.0 Flash)
# - Ground truth models: $0.02-0.10/doc (5-20x more expensive)
#
# Recommendation: Use ground truth models sparingly to create reference datasets,
# then test production models against those references.

# ====================
# Future Providers (Phase 1 - Week 4)
# ====================

# Moonshot AI (Kimi K2) - HIGH RISK: Chinese API, phone verification, VPN
# MOONSHOT_API_KEY=your_moonshot_api_key_here
# MOONSHOT_MODEL=moonshot-v1-128k

# Zhipu AI (ChatGLM) - HIGH RISK: Chinese API, approval process
# ZHIPU_API_KEY=your_zhipu_api_key_here
# ZHIPU_MODEL=glm-4

# ====================
# Optional Configuration
# ====================

# File paths (defaults work for most users)
# INPUT_DIR=input
# OUTPUT_DIR=output

# Performance timing (captures Docling + LLM extraction duration)
ENABLE_PERFORMANCE_TIMING=true

# ====================
# Prompt Version Selection (A/B Testing)
# ====================
# Toggle between baseline (V1) and enhanced (V2) extraction prompts
# V1 (default): Current production prompt - balanced extraction
# V2 (enhanced): Improved granularity and recall (2025-10-11)
#   - Date granularity rule: Separate event per distinct dated action
#   - Interim milestones guidance: Inspections, negotiations, status updates
#   - Limited details handling: Create events even with partial info
#   - Refined flexibility: 1-8 sentences as appropriate
#
# Expected improvements (based on 2025-10-03 benchmarks):
#   - Gemini: 83 events → 40-50 (reduced noise)
#   - Anthropic: 2 events → 6-10 (better recall)
#   - OpenRouter/OpenAI: 4-6 events → 8-12 (more complete)
#   - Variance: 41x → 10x (improved consistency)
#
# USE_ENHANCED_PROMPT=false  # V1 (default baseline)
# USE_ENHANCED_PROMPT=true   # V2 (enhanced prompt)
#
# Rollback: Simply remove or set to false to revert to V1

# ====================
# Docling PDF Processing Configuration
# ====================
# ⚡ Performance optimization: Disable OCR for digital PDFs (16x speedup)
# Benchmark results (2025-10-02): 35s → 2s per medium document
# Use DOCLING_DO_OCR=true only for scanned documents requiring OCR

# OCR Configuration (⭐ recommended: false for digital PDFs)
DOCLING_DO_OCR=false

# Auto-detect scanned PDFs and enable OCR automatically (⭐ NEW feature)
# When enabled: Checks PDF text layer, auto-enables OCR only for scanned docs
# Result: Digital PDFs stay fast (2s), scanned PDFs get OCR (22s with Tesseract)
DOCLING_AUTO_OCR_DETECTION=true

# OCR Engine Selection (⭐ NEW - 2025-10-03 benchmark: docs/benchmarks/2025-10-03-ocr-engine-war.md)
# ⭐ RECOMMENDED: Tesseract (3x faster than EasyOCR, 31% more text, production-ready)
# Options: tesseract (default), easyocr, ocrmac (macOS only), rapidocr (fast but low quality)
# DOCLING_OCR_ENGINE=tesseract
#
# Tesseract Language Data Path (REQUIRED when using tesseract engine)
# macOS (Homebrew): export TESSDATA_PREFIX=/usr/local/opt/tesseract/share/tessdata
# Linux (apt): export TESSDATA_PREFIX=/usr/share/tesseract-ocr/4.00/tessdata
# Windows: export TESSDATA_PREFIX=C:\Program Files\Tesseract-OCR\tessdata
#
# Install Tesseract:
# macOS: brew install tesseract
# Linux: sudo apt install tesseract-ocr libtesseract-dev
# Windows: Download from https://github.com/UB-Mannheim/tesseract/wiki

# Table Processing (keep enabled for legal documents)
DOCLING_DO_TABLE_STRUCTURE=true
DOCLING_TABLE_MODE=FAST
DOCLING_DO_CELL_MATCHING=true

# Advanced Configuration (defaults usually work)
# DOCLING_BACKEND=default
# DOCLING_ACCELERATOR_DEVICE=cpu
# DOCLING_ACCELERATOR_THREADS=4
# DOCLING_DOCUMENT_TIMEOUT=300

# For detailed provider comparison, pricing, and setup guides, see:
# docs/reference/configuration.md

# ====================
# OCR Engine War (benchmark script) settings
# ====================
# Rasterization and page budget for scripts/ocr_engine_war.py
# OCR_ENGINE_DPI=300
# OCR_ENGINE_PAGE_LIMIT=2

# Tesseract runtime (used by tesserocr)
# TESSERACT_LANGS=eng
# TESSERACT_OEM=1     # LSTM-only
# TESSERACT_PSM=6     # Assume a block of text
# TESSDATA_PREFIX=/usr/share/tesseract-ocr/4.00/tessdata

# PaddleOCR runtime
# PADDLEOCR_LANG=en
# PADDLEOCR_USE_ANGLE_CLS=true
